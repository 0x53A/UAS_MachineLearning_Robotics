{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "4a28be96",
      "metadata": {},
      "source": [
        "1 - Hallo\n",
        "\n",
        "2 - 1x Neuron in Python (vs Matlab)\n",
        "\n",
        "- Einschub: Erklärung AI Verwendung\n",
        "\n",
        "3 - Numerisch vs Symbolisch\n",
        "\n",
        "4 - Lineare Funktion, Vergleich Aktivierungsfunktionen\n",
        "\n",
        "  * Vergleich der Math. Gleichungen\n",
        "\n",
        "5 - Lineare Funktion mit Offset, Vergleich mit und ohne Bias\n",
        "\n",
        "  * Vergleich der Math. Gleichungen\n",
        "\n",
        "6 - nichtlineare Funktion\n",
        "\n",
        "  * zuerst den Vergleich mit n Layern m Neuronen\n",
        "\n",
        "  * dann \"universal approximation theorem\": alle kontinuierlichen! funktionen können mit einem Layer approximiert werden! (Das Theorem macht keine Aussage, wie einfach das zu trainieren ist, oder die Anzahl an Neuronen in dem einen Layer)\n",
        "\n",
        "  * dann 1 Layer 1 - 100 Neuronen\n",
        "\n",
        "  * \n",
        "\n",
        "7 - Performance Vergleich simple selbstgeschrieben vs Framework (Tinygrad) mit adaptiver Learn Rate"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6f52f6e",
      "metadata": {},
      "source": [
        "# 1 Neuron in Python\n",
        "\n",
        "siehe ./01_one-neuron.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c90dc15",
      "metadata": {},
      "source": [
        "## Verwendung von AI (LLMs)\n",
        "\n",
        "wir haben das eine Neuron komplett selber geschrieben.\n",
        "Für die komplexeren Netze haben wir für zwei Gewichte die Ableitungen mit der Hand berechnet, und danach entschieden dass wir einen Codegenerator schreiben (lassen).\n",
        "\n",
        "Der Code wurde größtenteils LLM generiert, nach eigenen Anforderungen."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8b262c1",
      "metadata": {},
      "source": [
        "# Numerische Ableitung vs Symbolische Ableitung\n",
        "\n",
        "Das Beispielprogramm berechnet die Ableitung numerisch, indem der Parameter leicht variiert wird, und die Auswirkung auf das Ergebnis beobachtet wird.\n",
        "Wir verwenden eine Pythonbibliothek (SymPi) um symbolisch abzuleiten.\n",
        "\n",
        "Zeigen: SymbolicLoss::mse()\n",
        "\n",
        "Zeigen: Netz-Deklaration (06-linear.ipynb, Topology Comparison)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09eeb92d",
      "metadata": {},
      "source": [
        "# Erstes Netz, lineare Funktion(en)\n",
        "\n",
        "```py\n",
        "def target_function(q1, q2):\n",
        "    X1 = q1 + q2\n",
        "    X2 = q1 - q2\n",
        "    return X1, X2\n",
        "```\n",
        "\n",
        "06-linear.pynb, Vergleich Layers und Neuronen\n",
        "\n",
        "Sieht cool aus, ist aber ziemlich Scheiße. Warum? Lineare Funktionen, Versuch mit nichtlinearer Aktivierungsfunktion ==> geht nicht\n",
        "\n",
        "Besser: lineare Akt. (identity) => 06b-linear.ipynb\n",
        "\n",
        "Ergebnis: (weights auslesen, gleichung ausgeben)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a13e3208",
      "metadata": {},
      "source": [
        "# Zweites Netz, lineare Funktion mit Offset \n",
        "\n",
        "```py\n",
        "def target_function(q1, q2):\n",
        "    X1 = q1 + q2 - 1\n",
        "    X2 = q1 - q2 + 1\n",
        "    return X1, X2\n",
        "```\n",
        "\n",
        "Zuerst: 07-offset-no-bias.ipynb\n",
        "\n",
        "Dann: 07b-offset.ipynb\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "880f0b1a",
      "metadata": {},
      "source": [
        "# Drittes Netz, nichtlineare Funktion\n",
        "\n",
        "```py\n",
        "def target_function(q1, q2):\n",
        "    X1 = q1 + (q2**2) - 1\n",
        "    X2 = q1 - q2 + 1\n",
        "    return X1, X2\n",
        "```\n",
        "\n",
        "Zuerst: Identity Akt. Funk.: 08b-nonlinear.ipynb\n",
        "Dann: ReLu: 08-nonlinear.ipynb\n",
        "Runter bis 10^-4"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c0fe1b3",
      "metadata": {},
      "source": [
        "# universal approximation theorem\n",
        "\n",
        "alle kontinuierlichen (!) funktionen können mit einem einzigen Layer approximiert werden! (Das Theorem macht keine Aussage, wie einfach das zu trainieren ist, oder die Anzahl an Neuronen in dem einen Layer)\n",
        "\n",
        "1 Layer, 8 Neuronen: 10^-5.5"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a48fbf74",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "8cee4433",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "uas-ml",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
