{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f301c35",
   "metadata": {},
   "source": [
    "q: Input\n",
    "\n",
    "x: neuron value / output\n",
    "\n",
    "w: weight\n",
    "\n",
    "b: bias\n",
    "\n",
    "z: neuron-internal value fed into the activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9288026",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# random input\n",
    "q1 = np.random.uniform(-1, 1, 1_000)\n",
    "q2 = np.random.uniform(-1, 1, 1_000)\n",
    "\n",
    "def f(q1, q2):\n",
    "\n",
    "    # linear\n",
    "    X1 = q1 + q2 - 1\n",
    "    X2 = q1 - q2 + 1\n",
    "\n",
    "    # Nonlinear\n",
    "    #X1 = q1 + (q2 ** 2) - 1\n",
    "    #X2 = q1 - q2 + 1\n",
    "\n",
    "    return X1, X2\n",
    "\n",
    "X1, X2 = f(q1, q2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23dbe00d",
   "metadata": {},
   "source": [
    "## Define the Neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864ac2fe",
   "metadata": {},
   "source": [
    "## Network Structure Diagram\n",
    "\n",
    "Below is a diagram showing the structure of the neural network used in this notebook. The network consists of 4 layers (including input and output), each with 2 neurons, and all connections are fully connected between adjacent layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8ab401",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "# Define the network structure\n",
    "layers = [2, 2, 2, 2]  # 4 layers, 2 neurons each\n",
    "G = nx.DiGraph()\n",
    "\n",
    "# Add nodes for each neuron in each layer\n",
    "positions = {}\n",
    "labels = {}\n",
    "layer_gap = 2\n",
    "neuron_gap = 1.2\n",
    "for l, n_count in enumerate(layers):\n",
    "    for n in range(n_count):\n",
    "        node = f\"L{l}N{n+1}\"\n",
    "        G.add_node(node)\n",
    "        # Position: (layer, neuron)\n",
    "        positions[node] = (l * layer_gap, -n * neuron_gap + (n_count-1)*neuron_gap/2)\n",
    "        labels[node] = f\"{node}\"\n",
    "\n",
    "# Add edges (fully connected between adjacent layers)\n",
    "for l in range(len(layers)-1):\n",
    "    for n1 in range(layers[l]):\n",
    "        for n2 in range(layers[l+1]):\n",
    "            G.add_edge(f\"L{l}N{n1+1}\", f\"L{l+1}N{n2+1}\")\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "nx.draw(G, pos=positions, with_labels=True, labels=labels, node_size=1200, node_color=\"#aee1f9\", font_size=10, arrowsize=18, font_weight='bold')\n",
    "plt.title(\"Network Layer and Connection Structure\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fdd1641",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sigmoid(z):\n",
    "    return 1/(1 + np.exp(-z))\n",
    "\n",
    "def activation_func(z, i_layer):\n",
    "\n",
    "    if i_layer != 0 and i_layer != 2:\n",
    "       return z\n",
    "\n",
    "    #return sigmoid(z)\n",
    "    #return np.tanh(z)\n",
    "    return z\n",
    "    # if z < 0:\n",
    "    #     return z * 0.1\n",
    "    # return z\n",
    "\n",
    "def neuron(u, w, b, i_layer):\n",
    "    # u is a vector, w is a vector of same length, compute dot product\n",
    "    z = np.dot(u, w) + b\n",
    "    x = activation_func(z, i_layer)\n",
    "    return x\n",
    "\n",
    "# q: input vector\n",
    "# w: weights matrix 3-dimensional (layer, neuron, weight)\n",
    "# b: bias matrix 2-dimensional (layer, neuron)\n",
    "def forward(q, w, b):\n",
    "    u = np.array(q)\n",
    "    for layer in range(len(w)):\n",
    "        u_next = []\n",
    "        for neuron_idx in range(len(w[layer])):\n",
    "            w_n = w[layer][neuron_idx]\n",
    "            b_n = b[layer][neuron_idx]\n",
    "            x = neuron(u, w_n, b_n, layer)\n",
    "            u_next.append(x)\n",
    "        u = np.array(u_next)\n",
    "    return u\n",
    "\n",
    "    \n",
    "def loss_func(x_ist, x_soll):\n",
    "    return np.sum((np.array(x_ist) - np.array(x_soll))**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032984ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def train(start_w, start_b, u_set, x_m_set, learning_rate, epochs):\n",
    "    w = copy.deepcopy(start_w)\n",
    "    b = copy.deepcopy(start_b)\n",
    "    loss_history = []\n",
    "    \n",
    "    for _epoch in range(epochs):\n",
    "\n",
    "\n",
    "        w_for_epoch = copy.deepcopy(w)\n",
    "        b_for_epoch = copy.deepcopy(b)\n",
    "\n",
    "        loss_for_epoch = 0\n",
    "\n",
    "\n",
    "        # for each epoch, train on all inputs\n",
    "        for i in range(len(u_set)):\n",
    "            u = u_set[i]\n",
    "            x_m = x_m_set[i]\n",
    "\n",
    "            # forward prop\n",
    "            x = forward(u, w, b)\n",
    "            loss = loss_func(x, x_m)\n",
    "\n",
    "            loss_for_epoch += loss\n",
    "\n",
    "            epsilon = 1e-6\n",
    "\n",
    "            # iterate over layers\n",
    "            for i_1 in range(len(w)):\n",
    "                # neurons\n",
    "                for i_2 in range(len(w[i_1])):\n",
    "                    # connections\n",
    "                    for i_3 in range(len(w[i_1][i_2])):\n",
    "                        # wiggle w a little bit\n",
    "                        w_perturbed = copy.deepcopy(w)\n",
    "                        w_perturbed[i_1][i_2][i_3] += epsilon\n",
    "                        x_perturbed = forward(u, w_perturbed, b)\n",
    "                        loss_perturbed_pos = loss_func(x_perturbed, x_m)\n",
    "\n",
    "                        w_perturbed = copy.deepcopy(w)\n",
    "                        w_perturbed[i_1][i_2][i_3] -= epsilon\n",
    "                        x_perturbed = forward(u, w_perturbed, b)\n",
    "                        loss_perturbed_neg = loss_func(x_perturbed, x_m)\n",
    "\n",
    "                        grad_w = (loss_perturbed_pos - loss_perturbed_neg) / (2 * epsilon)\n",
    "                        w_for_epoch[i_1][i_2][i_3] -= learning_rate * grad_w\n",
    "                    \n",
    "                    \n",
    "\n",
    "                    # wiggle b\n",
    "                    b_perturbed = copy.deepcopy(b)\n",
    "                    b_perturbed[i_1][i_2] += epsilon\n",
    "                    x_perturbed = forward(u, w, b_perturbed)\n",
    "                    loss_perturbed_pos = loss_func(x_perturbed, x_m)\n",
    "\n",
    "                    b_perturbed = copy.deepcopy(b)\n",
    "                    b_perturbed[i_1][i_2] -= epsilon\n",
    "                    x_perturbed = forward(u, w, b_perturbed)\n",
    "                    loss_perturbed_neg = loss_func(x_perturbed, x_m)\n",
    "\n",
    "                    grad_b = (loss_perturbed_pos - loss_perturbed_neg) / (2 * epsilon)\n",
    "                    b_for_epoch[i_1][i_2] -= learning_rate * grad_b\n",
    "                    \n",
    "        # after training on all inputs, update w and b\n",
    "        w = w_for_epoch\n",
    "        b = b_for_epoch\n",
    "\n",
    "        loss_history.append(loss_for_epoch)\n",
    "    \n",
    "    return w, b, loss_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824e78a3",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5909f498",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "start_w = [\n",
    "    #np.random.uniform(-1, 1, (2, 2)),  # input layer: 2 neurons, 2 inputs each\n",
    "    np.random.uniform(-1, 1, (2, 2)),  # hidden layer: 2 neurons, 2 inputs each\n",
    "    np.random.uniform(-1, 1, (2, 2)),  # hidden layer: 2 neurons, 2 inputs each\n",
    "    np.random.uniform(-1, 1, (2, 2))   # output layer: 2 neurons, 2 inputs each\n",
    "    \n",
    "    # np.zeros((2, 2)),\n",
    "    # np.zeros((2, 2)),\n",
    "    # np.zeros((2, 2)),\n",
    "    # np.zeros((2, 2))\n",
    "]\n",
    "\n",
    "# Bias hardcoded to 0\n",
    "start_b = [\n",
    "    #np.zeros(2),  # input layer biases\n",
    "    np.zeros(2),  # hidden layer biases\n",
    "    np.zeros(2),  # hidden layer biases\n",
    "    np.zeros(2)   # output layer biases\n",
    "]\n",
    "\n",
    "# input\n",
    "q = [[q1[i], q2[i]] for i in range(len(q1))]\n",
    "\n",
    "# output:\n",
    "x_m = [[X1[i], X2[i]] for i in range(len(q1))]\n",
    "\n",
    "w, b, loss_history = train(start_w, start_b, q, x_m, learning_rate=0.1, epochs=10)\n",
    "\n",
    "print(f\"Trained weight: {w}\")\n",
    "print(f\"Trained bias: {b}\")\n",
    "print(f\"Final loss: {loss_history[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85eaae58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_loss(loss_history):\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(loss_history)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Loss Over Time During Training')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.yscale('log')  # Log scale helps visualize the convergence better\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677f6459",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_loss(loss_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba55883",
   "metadata": {},
   "source": [
    "# Error over Input Parameter Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff729b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "# Create a grid over the input space\n",
    "grid_size = 100\n",
    "q1_range = np.linspace(-1, 1, grid_size)\n",
    "q2_range = np.linspace(-1, 1, grid_size)\n",
    "Q1_grid, Q2_grid = np.meshgrid(q1_range, q2_range)\n",
    "\n",
    "# Calculate error at each point\n",
    "error_grid = np.zeros_like(Q1_grid)\n",
    "\n",
    "for i in range(grid_size):\n",
    "    for j in range(grid_size):\n",
    "        test_q1, test_q2 = Q1_grid[i, j], Q2_grid[i, j]\n",
    "        test_input = [test_q1, test_q2]\n",
    "        \n",
    "        # Get network output\n",
    "        output = forward(test_input, w, b)\n",
    "        \n",
    "        # Calculate expected values (linear case)\n",
    "        expected_X1, expected_X2 = f(test_q1, test_q2)\n",
    "        \n",
    "        # Calculate total error (sum of squared errors)\n",
    "        error = (output[0] - expected_X1)**2 + (output[1] - expected_X2)**2\n",
    "        error_grid[i, j] = error\n",
    "\n",
    "# Create interactive 3D plot with Plotly\n",
    "fig = go.Figure(data=[go.Surface(\n",
    "    x=Q1_grid,\n",
    "    y=Q2_grid,\n",
    "    z=error_grid,\n",
    "    colorscale='Viridis',\n",
    "    colorbar=dict(title='Error')\n",
    ")])\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Network Error Over Input Parameter Space',\n",
    "    scene=dict(\n",
    "        xaxis_title='X1 (q1)',\n",
    "        yaxis_title='X2 (q2)',\n",
    "        zaxis_title='Î´ (Error)',\n",
    "        #zaxis_type='log'\n",
    "    ),\n",
    "    width=800,\n",
    "    height=700\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "print(f\"Max error: {np.max(error_grid):.6f}\")\n",
    "print(f\"Min error: {np.min(error_grid):.6f}\")\n",
    "print(f\"Mean error: {np.mean(error_grid):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a472a9",
   "metadata": {},
   "source": [
    "# Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc8e5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_q1, test_q2 = 0.5, 0.3\n",
    "test_input = [test_q1, test_q2]\n",
    "output = forward(test_input, w, b)\n",
    "\n",
    "# Expected values based on the training targets\n",
    "expected_X1, expected_X2 = f(test_q1, test_q2)\n",
    "\n",
    "print(f\"Input: q1={test_q1}, q2={test_q2}\")\n",
    "print(f\"Network output: X1={output[0]:.4f}, X2={output[1]:.4f}\")\n",
    "print(f\"Expected:       X1={expected_X1:.4f}, X2={expected_X2:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uas-ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
